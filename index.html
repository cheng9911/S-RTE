<!doctype html>
<meta charset=utf-8>

<head>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet" type="text/css">
    <link href="css/misc.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css2?family=Inter&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', '');
    </script>
</head>

<title>S-RTE #39</title>

<div class="col-sm-12" style="text-align: center;">
    <h1 class="name" style="font-family:'Inter';font-weight: 500;"><span>Skill-Aware Robotic Task Execution with
        </span><span>
            Executable-Semantic Descriptors and CLIP-LLM Planning</span><br>
        <!-- <h1 style="color:#5a6268;font-size: 1.25em;">*****</h1><br> -->

        <div class="container my-2">
            <div class="row justify-content-center">
                <div class="col-auto">
                    <a class="btn btn-light border rounded-pill px-4 py-2 shadow-sm mx-1" href="" target="_blank"
                        rel="noopener">arXiv</a>
                </div>
                <div class="col-auto">
                    <a class="btn btn-light border rounded-pill px-4 py-2 shadow-sm mx-1" href="" target="_blank"
                        rel="noopener">paper</a>
                </div>
                <div class="col-auto">
                    <a class="btn btn-secondary border rounded-pill px-4 py-2 shadow-sm mx-1 disabled" href="#"
                        onclick="alert('Code coming soon!')" style="pointer-events: auto; cursor: pointer;">
                        Skill Perception Code (Coming Soon)
                    </a>
                </div>

                <div class="col-auto">
                    <a class="btn btn-secondary border rounded-pill px-4 py-2 shadow-sm mx-1 disabled" href="#"
                        onclick="alert('Code coming soon!')" style="pointer-events: auto; cursor: pointer;">
                        Skill Collection Code (Coming Soon)
                    </a>
                </div>
            </div>
        </div>

        <div class="is-size-5 publication-authors">
            <!-- <span class="author-block">
            Jincheng Sun</a><sup>*1,2</sup>,</span> -->
            <!-- <span class="author-block">
                <a href="https://www.jcsun.cc/">Jincheng Sun</a><sup>*1</sup>,</span>
            <span class="author-block">
                <a href="https://yluo.name/">Yang Luo</a><sup>*1,2</sup>,</span> -->
            <!-- <span class="author-block">
                *****</a><sup>1,2</sup>,</span>
            <span class="author-block">
                *****</a><sup>1</sup>,</span>
            <span class="author-block"> -->
            <!-- <a href="https://c7w.tech/">Huan-ang Gao</a><sup>1</sup>,</span>
            <span class="author-block">
            <a href="https://ziweiwangthu.github.io">Ziwei Wang</a><sup>3</sup>,</span>
            <span class="author-block">
            <a href="https://sites.google.com/view/fromandto">Hao Zhao</a><sup>^1,2</sup></span> -->

        </div>

        <div class="is-size-5 publication-authors"></div><br />
        <!-- <span class="author-block"><sup>*</sup>Equal Contribution</span>
        <span class="author-block"><sup>^</sup>Corresponding Author</span><br />
        <span class="author-block"><sup>1</sup>College of Information Science and Engineering, Northeastern University,
            Shenyang 110004, China</span><br />
        <span class="author-block"><sup>2</sup>State Key Laboratory of Robotics, Shenyang Institute of
            Automation,Chinese Academy of Sciences, Shenyang 110016,China</span><br /> -->
        <!-- <span class="author-block"><sup>3</sup>Nanyang Technological University</span><br/> -->

</div>
</div>

<!-- Table of Contents -->
<!-- <div class="col-sm-12" style="font-family:'Helvetica';text-align: center;">
    <a style="font-family:'Inter';font-size:1.25em;">Paper Submission #39</a>
    <br>
    <a style="font-family:'Inter';font-size:1.15em;color:#4B9AE7">Anonymous Author(s)</a><br>

    <a style="font-family:'Inter';font-size:1.05em;">Affiliation</a><br>

</div> -->

<!-- <div class="main"> -->
<section class="section" id="Abstract">
    <div class="container text-center">
        <div class="container">

            <hr>
            <h1 style="text-align:center; margin-top: 0pt; margin-bottom: 10pt;font-family:'Inter';">Abstract</h1>

            <div class="col-12 text-center">
                <p style="text-align:left;color: #000000" ;font-size: 0.7em;>
                    In dynamic unstructured environments, achieving safe, scalable, and interpretable robotic skill
                    execution remains a core bottleneck toward general-purpose embodied intelligence. Existing
                    vision-language-action (VLA) paradigms typically employ end-to-end large models that map high-level
                    task semantics directly to low-level action sequences. Their black-box nature limits
                    interpretability, reduces skill reuse, and fails to provide deterministic guarantees in
                    safety-critical scenarios. To overcome these limitations, we propose the Skill-Aware Robotic Task
                    Execution(S-RTE) framework, which establishes a skill perception-reasoning-execution loop for
                    gray-box and safety-aware control. At its core, we introduce the Skill-Aware Executable-Semantic
                    Descriptor (S-ESD), a unified representation that bridges low-level control APIs with high-level
                    semantic descriptions, enabling structured skill retrieval and composition by large language models
                    (LLMs). Building upon this representation, we design the Skill Perception Network (S-ESD-CLIP) that
                    leverages vision-language embedding alignment to perceive the executability and progress status (not
                    started / in progress / completed) of candidate skills in real time, effectively pruning unsafe or
                    infeasible skills. Finally, a gray-box planner (CLIP-LLM) integrates perceived feasibility cues as
                    constraints into the reasoning process, producing interpretable “skill-condition” plans that support
                    dynamic task decomposition and closed-loop execution. Extensive real-world experiments demonstrate
                    that S-RTE significantly improves safety, robustness, and skill scalability, while maintaining
                    real-time inference on a single NVIDIA Jetson. The proposed framework offers a promising pathway
                    toward deployable and interpretable embodied intelligence.
                </p>
            </div>
        </div>
        <div class="container text-center my-4">
            <!-- 图片部分 -->
            <div class="row justify-content-center mb-3">
                <div class="col-10">
                    <img src="./videos/srte.jpg" alt="S-RTE Framework Overview" class="img-fluid rounded shadow">
                </div>
            </div>

            <!-- 文字部分 -->
            <div class="row justify-content-center">
                <div class="col-10">
                    <p class="text-left" style="font-size: 1rem; line-height: 1.6;">
                        <strong style="color:black; font-weight:900;">Overview of the S-RTE framework.</strong>
                        (<strong style="color:black; font-weight:900;">a) Skill-Aware Executable-Semantic Descriptor
                            (ESD):</strong>
                        Each skill is formalized as a structured five-tuple ⟨ID, Description, Model, State, Tag⟩,
                        providing a unified representation of semantic abstraction and low-level control. Skills are
                        acquired via imitation learning (IL), or end-to-end training, and stored in a modular skill
                        library for retrieval and reuse.
                        (<strong style="color:black; font-weight:900;">b) S-ESD with CLIP (S-ESD-CLIP):</strong>
                        A lightweight multimodal perception module leveraging CLIP-based vision-language alignment
                        evaluates the environmental executability of candidate skills in real time and updates
                        stage-aware execution states (“not started,” “in progress,” “completed”), enabling dynamic
                        pruning of infeasible actions.
                        (<strong style="color:black; font-weight:900;">c) CLIP-Guided Large Language Model Planner
                            (S-CLIP-LLM):</strong>
                        High-level task planning incorporates the filtered skill set from S-ESD-CLIP as prior
                        constraints for the LLM, generating sequential task plans with interpretable reasoning. The
                        bottom-right illustration demonstrates two composite tasks—Task 1 (“place bottle → basket”) and
                        Task 2 (“place cookie → plate”)—showing multi-skill sequential execution under closed-loop
                        supervision.
                    </p>
                </div>
            </div>
        </div>


</section><br>
<section class="section" id="Skill Perception Evaluation">
    <div class="container text-center">
        <div class="container">

            <hr>
            <h1 style="text-align:center; margin-top: 0pt; margin-bottom: 10pt;font-family:'Inter';">Skill Perception
                Evaluation</h1>

            <div class="col-12 text-center">
                <p style="text-align:left">
                    In this section,We evaluate the S-ESD-CLIP model's skill perception using a bottle grasping task as
                    a reference skill. The model predicts skill feasibility and execution stage under three experimental
                    conditions: standard, cluttered, and occluded environments.
                </p>
            </div>

            <div class="row justify-content-center mb-3">
                <div class="col-10">
                    <img src="./videos/S-ESD-CLIP.jpg" alt="S-RTE Framework Overview" class="img-fluid rounded shadow">
                </div>
            </div>

            <!-- 文字部分 -->
            <div class="row justify-content-center">
                <div class="col-10">
                    <p class="text-left" style="font-size: 1rem; line-height: 1.6;">
                        <strong style="color:black; font-weight:900;">Overview of the S-ESD-CLIP framework.</strong>
                        (<strong style="color:black; font-weight:900;">(a) Pre-training of S-ESD-CLIP:</strong>
                        Textual skill descriptions and visual observations are independently encoded by text and image
                        encoders,
                        and their similarity is computed through the CLIP model followed by softmax normalization.
                        (<strong style="color:black; font-weight:900;">(b) ESD-CLIP for skill selection:</strong>
                        The module integrates current observations with the skill library to dynamically generate a
                        subset of executable skills that guide downstream task execution. This mechanism ensures that
                        only contextually feasible skills are selected, thereby enhancing the reliability of high-level
                        planning and execution.


                    </p>
                </div>
            </div>
        </div>





        <style>
            .no-gutters {
                margin-right: 0;
                margin-left: 0;
            }

            .no-gutters>[class*="col-"] {
                padding-right: 0;
                padding-left: 0;
            }

            video {
                width: 100% !important;
                height: auto !important;
                display: block;
            }

            .video-title {
                text-align: center;
                /* 居中题目 */
                margin-top: 0.5rem;
                /* 跟视频留一点空隙 */
                font-size: 1rem;
                color: #333;
            }
        </style>

        <div class="container text-center">
            <div class="row no-gutters my-4">
                <div class="col-4">
                    <video controls loop autoplay muted src="./videos/Standard_environment.mp4"></video>
                    <p class="video-title">Standard environment</p>
                </div>
                <div class="col-4">
                    <video controls loop autoplay muted src="./videos/Cluttered_env.mp4"></video>
                    <p class="video-title">Cluttered environment </p>
                </div>
                <div class="col-4">
                    <video controls loop autoplay muted src="./videos/Occluded_scenes.mp4"></video>
                    <p class="video-title">Cluttered environment </p>
                </div>
                <!-- <div class="col-4">
      <video controls loop autoplay muted src="./videos/torque_demonstration/Button_Pushing.mp4"></video>
      <p class="video-title">Button Pushing</p>
    </div> -->
            </div>
        </div>

        <section class="section" id="LLM-guided hierarchical task planning">
            <div class="container text-center">
                <div class="container">

                    <hr>
                    <h1 style="text-align:center; margin-top: 0pt; margin-bottom: 10pt;font-family:'Inter';">LLM-guided
                        hierarchical task planning</h1>

                    <div class="col-12 text-center">
                        <p style="text-align:left">
                            In this section, we present S-CLIP-LLM, a CLIP-guided Large Language Model Planner that
                            integrates the dynamically filtered skill library from S-ESD-CLIP with LLM reasoning. This
                            enables context-aware, goal-driven long-horizon task planning through semantically valid
                            skill sequencing.
                    </div>

                    <div class="row justify-content-center mb-3">
                        <div class="col-10">
                            <img src="./videos/LLM_planer.jpg" alt="S-RTE Framework Overview"
                                class="img-fluid rounded shadow">
                        </div>
                    </div>

                    <!-- 文字部分 -->
                    <div class="row justify-content-center">
                        <div class="col-10">
                            <p class="text-left" style="font-size: 1rem; line-height: 1.6;">
                                <strong style="color:black; font-weight:900;">Overview of the S-CLIP-LLM
                                    framework.</strong>
                                <strong style="color:black; font-weight:900;">(a) Real-time skill perception with
                                    S-ESD-CLIP: </strong>
                                The module dynamically filters the skill library based on current observations, aligning
                                visual context with textual skill representations to identify executable skills in real
                                time.
                                <strong style="color:black; font-weight:900;">(b) LLM-guided hierarchical task
                                    planning:</strong>
                                The CLIP-Guided Large Language Model Planner integrates the filtered skill set with LLM
                                reasoning to generate semantically valid and physically feasible skill sequences. By
                                considering skill execution states (“not started,” “in progress,” “completed”), it
                                enables interpretable, safe, and context-aware long-horizon task execution.

                        </div>
                    </div>
                </div>





                <style>
                    .no-gutters {
                        margin-right: 0;
                        margin-left: 0;
                    }

                    .no-gutters>[class*="col-"] {
                        padding-right: 0;
                        padding-left: 0;
                    }

                    video {
                        width: 100% !important;
                        height: auto !important;
                        display: block;
                        border-radius: 10px;
                    }

                    .video-title {
                        text-align: center;
                        margin-top: 0.5rem;
                        font-size: 1rem;
                        color: #333;
                    }
                </style>

                <div class="container text-center">
                    <div class="row justify-content-center no-gutters my-4">
                        <div class="col-8">
                            <video controls loop autoplay muted src="./videos/software/soft.mp4"></video>
                            <p class="video-title">Perception + LLM Hierarchical Planning</p>
                        </div>
                    </div>
                </div>


                <section class="section" id="ACT vs. ACT+LLM vs. Ours">
                    <div class="container text-center">
                        <div class="container">

                            <hr>
                            <h1 style="text-align:center; margin-top: 0pt; margin-bottom: 10pt;font-family:'Inter';">ACT
                                vs. ACT+LLM vs. Ours</h1>

                            <div class="col-12 text-center">
                                <p style="text-align:left">
                                    From the failure cases of traditional ACT and ACT+LLM, it becomes evident that
                                    real-time skill perception is essential for reliable task execution. Our
                                    perception-integrated planner combines visual understanding with language reasoning
                                    to detect failures, adapt on the fly, and achieve consistent long-horizon
                                    performance.
                                </p>
                            </div>
                            <div class="col-12 text-center">
                                <h4 style="font-weight:700; margin-bottom: 1rem;">Tier 1 – Single Skill</h4>
                                <p style="text-align: left;">
                                    In this baseline task, the robot places a bottle in the basket, testing fundamental
                                    manipulation and motion control. The results serve as a calibration reference before
                                    progressing to more complex multi-skill or disturbed scenarios.
                                </p>
                            </div>


                            <div class="container text-center">
                                <div class="row align-items-center my-4">
                                    <div class="col-lg-2 col-md-2 col-sm-2 col-2 text-center">
                                        <h5 class="mt-2"> places a bottle in the basket</h5>
                                    </div>
                                    <div class="col-lg-1 col-md-1 col-sm-1 col-1 text-center"></div>

                                    <div class="col-lg-9 col-md-9 col-sm-9 col-9 text-center">
                                        <div class="embed-responsive embed-responsive-16by9">
                                            <video controls loop autoplay muted>
                                                <source src="./videos/base/input2.mp4" type="video/mp4">
                                            </video>
                                        </div>
                                    </div>
                                </div>
                            </div>



                            <div class="col-12 text-center">
                                <h4 style="font-weight:700; margin-bottom: 1rem;">Tier 2 – Sequential Composition</h4>
                                <p style="text-align:left">
                                    In this tier, the system must execute a two-step sequence: place the bottle in the
                                    basket, then place the cookie on the plate. This stage tests the planner’s ability
                                    to maintain state awareness and perform long-horizon skill coordination, ensuring
                                    smooth and correct transitions between atomic skills.
                                </p>
                            </div>
                            <div class="container text-center">
                                <div class="row align-items-center my-4">
                                    <div class="col-lg-2 col-md-2 col-sm-2 col-2 text-center">
                                        <h5 class="mt-2">place the bottle in the
                                            basket, then place the cookie on the plate</h5>
                                    </div>
                                    <div class="col-lg-1 col-md-1 col-sm-1 col-1 text-center"></div>

                                    <div class="col-lg-9 col-md-9 col-sm-9 col-9 text-center">
                                        <div class="embed-responsive embed-responsive-16by9">
                                            <video controls loop autoplay muted>
                                                <source src="./videos/Tier2/tier2.mp4" type="video/mp4">
                                            </video>
                                        </div>
                                    </div>
                                </div>
                            </div><br>


                            <div class="col-12 text-center">
                                <h4 style="font-weight:700; margin-bottom: 1rem;">
                                    Tier 3 – Unexpected Condition
                                </h4>

                                <p style="text-align:left">
                                    This tier examines the controller’s real-time adaptability and safety awareness
                                    under dynamic
                                    environmental disturbances. The task repeats the two-step sequence—placing the
                                    bottle in the
                                    basket and the cookie on the plate—but introduces unexpected perturbations such as a
                                    missing
                                    target object, manual scene restoration, or partial prior skill completion. A robust
                                    system
                                    should detect infeasible conditions and respond safely by pausing, re-evaluating, or
                                    re-planning
                                    instead of continuing an invalid trajectory.
                                </p>
                            </div>


                            <style>
                                .video-container {
                                    margin-bottom: 0.1rem;
                                }

                                .video-title {
                                    font-weight: 600;
                                    font-size: 1rem;
                                    margin-top: 0.5rem;
                                    color: #333;
                                }

                                .video-caption {
                                    font-size: 0.9rem;
                                    color: #666;
                                    margin-top: 0.25rem;
                                }

                                video {
                                    width: 100%;
                                    height: auto;
                                    border-radius: 10px;
                                }
                            </style>

                            <div class="container text-center my-0">
                                <!-- 总标题 -->
                                <h4 style="tier-title">
                                    Baseline Failures (ACT / ACT+LLM)
                                </h4>
                                <p style="color:#555;  ">
                                    Representative failure cases highlighting the limitations of traditional ACT and
                                    ACT+LLM under dynamic task conditions.
                                </p>

                                <!-- 三个视频并排 -->
                                <div class="row justify-content-center">
                                    <!-- Case 1 -->
                                    <div class="col-lg-4 col-md-6 col-sm-12 video-container">
                                        <video controls loop autoplay muted>
                                            <source src="./videos/Tier3/Missing_Object.mp4" type="video/mp4">
                                        </video>
                                        <p class="video-title">Case 1 – Missing Object</p>
                                        <p class="video-caption">The system fails to detect that the target object has
                                            been removed.</p>
                                    </div>

                                    <!-- Case 2 -->
                                    <div class="col-lg-4 col-md-6 col-sm-12 video-container">
                                        <video controls loop autoplay muted>
                                            <source src="./videos/Tier3/input2.mp4" type="video/mp4">
                                        </video>
                                        <!-- <video controls loop autoplay muted playsinline>
                                            <video controls loop autoplay muted>
                                            <source src="./videos/Tier3/Incorrect.mp4" type="video/mp4">
                                        </video> -->
                                        <p class="video-title">Case 2 – Incorrect Skill Sequence</p>
                                        <p class="video-caption">ACT+LLM skips placing the bottle after detecting the
                                            cookie is already on the plate, leading to failure.</p>

                                    </div>

                                    <!-- Case 3 -->
                                    <div class="col-lg-4 col-md-6 col-sm-12 video-container">
                                        <video controls loop autoplay muted>
                                            <source src="./videos/Tier3/Failure_Under_Occlusion.mp4" type="video/mp4">
                                        </video>
                                        <p class="video-title">Case 3 – Failure Under Occlusion</p>
                                        <p class="video-caption">
                                            The system fails to adapt when the environment introduces occlusions,
                                            continuing execution without proper recovery.
                                        </p>
                                    </div>


                                </div>
                            </div>


                            <div class="container text-center my-0">
                                <!-- 总标题 -->
                                <h4 style="tier-title">
                                    Our Successful Executions (S-RTE)
                                </h4>
                                <p style="color:#555; ">
                                    Representative cases demonstrating the robustness, adaptability, and
                                    perception-guided planning of our S-RTE framework.
                                </p>

                                <!-- 三个视频并排 -->
                                <div class="row justify-content-center">
                                    <!-- Case 1 -->
                                    <div class="col-lg-4 col-md-6 col-sm-12 video-container">
                                        <video controls loop autoplay muted>
                                            <source src="./videos/Tier3/Ours_Case1.mp4" type="video/mp4">
                                        </video>
                                        <p class="video-title">Case 1 – Scene Manually Restored</p>
                                        <p class="video-caption">
                                            S-RTE adapts as an experimenter repositions objects, completing the task
                                            successfully.
                                        </p>
                                    </div>

                                    <!-- Case 2 -->
                                    <div class="col-lg-4 col-md-6 col-sm-12 video-container">
                                        <video controls loop autoplay muted>
                                            <source src="./videos/Tier3/Ours_Case2.mp4" type="video/mp4">
                                        </video>
                                        <p class="video-title">Case 2 – Correct Skill Sequencing</p>
                                        <p class="video-caption">
                                            S-RTE properly maintains skill states, placing the bottle and cookie in the
                                            correct order.
                                        </p>
                                    </div>

                                    <!-- Case 3 -->
                                    <div class="col-lg-4 col-md-6 col-sm-12 video-container">
                                        <video controls loop autoplay muted>
                                            <source src="./videos/Tier3/Ours_Case3.mp4" type="video/mp4">
                                        </video>
                                        <p class="video-title">Case 3 – Adaptation Under Occlusion</p>
                                        <p class="video-caption">
                                            S-RTE successfully adapts when objects are occluded, completing the task
                                            without failure.
                                        </p>
                                    </div>
                                </div>
                            </div>



                </section><br>




                <section class="section" id="Acknowledgements">
                    <div class="container text-center">
                        <div class="container">

                            <hr>
                            <h1 style="text-align:center; margin-top: 0pt; margin-bottom: 10pt;font-family:'Inter';">
                                Citation</h1>
                            <p style="text-align:left">
                                We kindly request that you cite our work if you utilize the code or reference our
                                findings in your research.</p>
                            <div class="col-12 text-center">
                                <pre style="text-align:left;color: #aaaaaa" ;font-size: 0.7em; class="pre">
<!-- @article{
  title={Skill-Aware Robotic Task Execution with Executable-Semantic Descriptors and CLIP-LLM Planning },
  year={2025}
} -->
    </pre>
                            </div>
                        </div>
                        <style>
                            pre {
                                white-space: pre-wrap;
                                white-space: -moz-pre-wrap;
                                white-space: -pre-wrap;
                                white-space: -o-pre-wrap;
                                word-wrap: break-word;
                            }
                        </style>